// SPDX-License-Identifier: AGPL-3.0-or-later
// Â© 2025 Ryo âˆ´ SpiralArchitect (kishkavsesvit@icloud.com)
// Part of SpiralTorch â€” Licensed under AGPL-3.0-or-later.

//! Example demonstrating custom operator registration.
//!
//! Shows how to extend SpiralTorch with domain-specific tensor operations
//! using the pluggable operator registry.

use st_core::ops::{global_operator_registry, OperatorBuilder};
use st_core::PureResult;
use st_tensor::Tensor;
use std::sync::Arc;

fn print_tensor(label: &str, tensor: &Tensor) {
    let (rows, cols) = tensor.shape();
    println!("{label} (shape=({rows},{cols})):");
    let data = tensor.data();
    for r in 0..rows {
        let start = r * cols;
        let end = start + cols;
        println!("  Row {r}: {:?}", &data[start..end]);
    }
}

fn main() -> PureResult<()> {
    println!("ğŸŒ€ Custom Operator Registration Example ğŸŒ€\n");

    let registry = global_operator_registry();

    // Register a custom element-wise square operator
    println!("ğŸ“ Registering custom operators...\n");

    let square_op = OperatorBuilder::new("square", 1, 1)
        .with_description("Element-wise square: y = x^2")
        .with_backend("CPU")
        .with_backend("WGPU")
        .with_backend("CUDA")
        .with_differentiable(true)
        .with_attribute("operation_type", "unary")
        .with_forward(Arc::new(|inputs| {
            println!("  ğŸ”§ Executing square operator");
            let x = inputs[0];
            let (rows, cols) = x.shape();
            
            let data: Vec<f32> = x
                .data()
                .iter()
                .map(|&v| v * v)
                .collect();
            
            Ok(vec![Tensor::from_vec(rows, cols, data)?])
        }))
        .with_backward(Arc::new(|inputs, _outputs, grad_outputs| {
            println!("  ğŸ”„ Computing square gradient");
            let x = inputs[0];
            let grad_y = grad_outputs[0];
            let (rows, cols) = x.shape();
            
            // Gradient: dy/dx = 2x
            let grad_data: Vec<f32> = x
                .data()
                .iter()
                .zip(grad_y.data().iter())
                .map(|(&x_val, &grad_val)| 2.0 * x_val * grad_val)
                .collect();
            
            Ok(vec![Tensor::from_vec(rows, cols, grad_data)?])
        }))
        .build()?;

    registry.register(square_op)?;
    println!("  âœ… Registered 'square' operator\n");

    // Register a custom normalization operator
    let normalize_op = OperatorBuilder::new("normalize", 1, 1)
        .with_description("L2 normalization: y = x / ||x||_2")
        .with_backend("CPU")
        .with_differentiable(true)
        .with_attribute("operation_type", "normalization")
        .with_forward(Arc::new(|inputs| {
            println!("  ğŸ”§ Executing normalize operator");
            let x = inputs[0];
            let (rows, cols) = x.shape();
            
            // Compute L2 norm
            let norm: f32 = x.data().iter().map(|&v| v * v).sum::<f32>().sqrt();
            let norm = norm.max(1e-8); // Avoid division by zero
            
            let data: Vec<f32> = x.data().iter().map(|&v| v / norm).collect();
            
            Ok(vec![Tensor::from_vec(rows, cols, data)?])
        }))
        .with_backward(Arc::new(|inputs, outputs, grad_outputs| {
            println!("  ğŸ”„ Computing normalize gradient");
            let x = inputs[0];
            let _y = outputs[0];
            let grad_y = grad_outputs[0];
            let (rows, cols) = x.shape();
            
            // Gradient computation for L2 normalization
            // d/dx (x / ||x||) = (I - xx^T/||x||^2) / ||x||
            let norm: f32 = x.data().iter().map(|&v| v * v).sum::<f32>().sqrt().max(1e-8);
            let norm_sq = norm * norm;
            
            // Compute x^T * grad_y (dot product)
            let dot_product: f32 = x
                .data()
                .iter()
                .zip(grad_y.data().iter())
                .map(|(&x_val, &grad_val)| x_val * grad_val)
                .sum();
            
            let grad_data: Vec<f32> = x
                .data()
                .iter()
                .zip(grad_y.data().iter())
                .map(|(&x_val, &grad_val)| {
                    (grad_val * norm - x_val * dot_product / norm) / norm_sq
                })
                .collect();
            
            Ok(vec![Tensor::from_vec(rows, cols, grad_data)?])
        }))
        .build()?;

    registry.register(normalize_op)?;
    println!("  âœ… Registered 'normalize' operator\n");

    // Register a custom binary operator: weighted sum
    let weighted_sum_op = OperatorBuilder::new("weighted_sum", 2, 1)
        .with_description("Weighted sum: y = a * x1 + (1-a) * x2")
        .with_backend("CPU")
        .with_differentiable(true)
        .with_attribute("operation_type", "binary")
        .with_forward(Arc::new(|inputs| {
            println!("  ğŸ”§ Executing weighted_sum operator");
            let x1 = inputs[0];
            let x2 = inputs[1];
            let (rows, cols) = x1.shape();
            
            // For simplicity, use a = 0.7
            let a = 0.7f32;
            let data: Vec<f32> = x1
                .data()
                .iter()
                .zip(x2.data().iter())
                .map(|(&v1, &v2)| a * v1 + (1.0 - a) * v2)
                .collect();
            
            Ok(vec![Tensor::from_vec(rows, cols, data)?])
        }))
        .with_backward(Arc::new(|_inputs, _outputs, grad_outputs| {
            println!("  ğŸ”„ Computing weighted_sum gradient");
            let grad_y = grad_outputs[0];
            let (rows, cols) = grad_y.shape();
            
            let a = 0.7f32;
            
            // Gradient w.r.t. x1: a * grad_y
            let grad_x1: Vec<f32> = grad_y.data().iter().map(|&g| a * g).collect();
            
            // Gradient w.r.t. x2: (1-a) * grad_y
            let grad_x2: Vec<f32> = grad_y.data().iter().map(|&g| (1.0 - a) * g).collect();
            
            Ok(vec![
                Tensor::from_vec(rows, cols, grad_x1)?,
                Tensor::from_vec(rows, cols, grad_x2)?,
            ])
        }))
        .build()?;

    registry.register(weighted_sum_op)?;
    println!("  âœ… Registered 'weighted_sum' operator\n");

    // List registered operators
    println!("ğŸ“‹ Registered operators:");
    for op_name in registry.list_operators() {
        if let Some(op) = registry.get(&op_name) {
            let meta = op.metadata();
            println!("  - {} ({})", op_name, meta.description);
            println!("    Inputs: {}, Outputs: {}", 
                     meta.signature.num_inputs, 
                     meta.signature.num_outputs);
            println!("    Backends: {}", meta.backends.join(", "));
            println!("    Differentiable: {}", meta.signature.differentiable);
        }
    }
    println!();

    // Test the operators
    println!("ğŸ§ª Testing custom operators...\n");

    // Create test tensors
    let x = Tensor::from_vec(2, 2, vec![1.0, 2.0, 3.0, 4.0])?;
    print_tensor("Input tensor x", &x);
    println!();

    // Test square operator
    let y_square = registry.execute("square", &[&x])?;
    print_tensor("After square", &y_square[0]);
    println!();

    // Test normalize operator
    let y_norm = registry.execute("normalize", &[&x])?;
    print_tensor("After normalize", &y_norm[0]);
    println!();

    // Test weighted sum operator
    let x2 = Tensor::from_vec(2, 2, vec![4.0, 3.0, 2.0, 1.0])?;
    let y_weighted = registry.execute("weighted_sum", &[&x, &x2])?;
    print_tensor("After weighted_sum", &y_weighted[0]);
    println!();

    // Test gradient computation
    println!("ğŸ”¬ Testing gradient computation...\n");
    
    let grad_output = Tensor::from_vec(2, 2, vec![1.0, 1.0, 1.0, 1.0])?;
    
    if let Some(square_op) = registry.get("square") {
        let y_square_refs: Vec<&Tensor> = y_square.iter().collect();
        let grad_input = square_op.backward(&[&x], &y_square_refs, &[&grad_output])?;
        print_tensor("Square gradient w.r.t. input", &grad_input[0]);
        println!();
    }

    // Find operators by backend
    println!("ğŸ” Finding operators by backend...\n");
    
    let cuda_ops = registry.find_by_backend("CUDA");
    println!("CUDA-enabled operators:");
    for op in cuda_ops {
        println!("  - {}", op.metadata().signature.name);
    }
    println!();

    let cpu_ops = registry.find_by_backend("CPU");
    println!("CPU-enabled operators:");
    for op in cpu_ops {
        println!("  - {}", op.metadata().signature.name);
    }
    println!();

    println!("âœ¨ Custom operator example completed successfully!\n");
    println!("Key features demonstrated:");
    println!("  âœ“ Custom operator registration");
    println!("  âœ“ Forward and backward implementations");
    println!("  âœ“ Multi-backend support");
    println!("  âœ“ Operator discovery and querying");
    println!("  âœ“ Gradient computation\n");

    Ok(())
}
